{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6086a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "from io import StringIO\n",
    "from io import BytesIO\n",
    "import requests\n",
    "import gzip\n",
    "from bs4 import BeautifulSoup as BS\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186f7618",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.crunchbase.com/www-sitemaps/sitemap-index.xml'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4638f854",
   "metadata": {},
   "source": [
    "# Modelo de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e3a57a",
   "metadata": {},
   "source": [
    "La dificultad de recoger datos de Crunchbase reside en que cada página tiene una serie de campos que no son necesariamente los mismos. Por lo tanto, hemos considerado que en este caso **el modelo de datos debe ser dinámico**, puesto que no conocemos de antemano qué variables vamos a almacenar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e2a21b",
   "metadata": {},
   "source": [
    "Hemos definido la siguiente función que mapea los campos informados de un perfil en concreto a un diccionario {clave: valor}, que añadiremos más tarde al conjunto de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b7ae9e",
   "metadata": {},
   "source": [
    "También hemos incluido un *log* que nos permite ver en tiempo real la información que se está recogiendo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea67fc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse2row(soup, log=False):\n",
    "    row = {}\n",
    "    print(\"---\") if log else None\n",
    "    try:\n",
    "        name = soup.select('h1[class=\"profile-name\"]')[0].text.strip()\n",
    "        row[\"Name\"] = name\n",
    "        print(\"Name: \" + name) if log else None\n",
    "        \n",
    "        profile = soup.select('div[class=\"profile-type\"] > span')[0].text\n",
    "        row[\"Profile\"] = profile\n",
    "        print(\"Profile: \" + profile) if log else None\n",
    "    except IndexError:\n",
    "        pass\n",
    "    \n",
    "    fields = soup.select('page-centered-layout .main-content profile-section .section-content fields-card > ul li')\n",
    "    for li in fields:\n",
    "        value = ''\n",
    "        try:\n",
    "            label = list(li.select('label-with-info')[0].stripped_strings)[0]\n",
    "            values = list(li.select('field-formatter')[0].stripped_strings)\n",
    "\n",
    "            try:\n",
    "                while True:\n",
    "                    values.remove(',')\n",
    "            except ValueError:\n",
    "                pass        \n",
    "            value = ', '.join(values)\n",
    "\n",
    "            row[label] = value\n",
    "            print(label + ': ' + value) if log else None\n",
    "        except IndexError:\n",
    "            pass\n",
    "\n",
    "    return row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff596eb",
   "metadata": {},
   "source": [
    "# Resolución de obstáculos en web scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a536e2",
   "metadata": {},
   "source": [
    "## Modificación del *user agent* y otras cabeceras HTTP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7753b9",
   "metadata": {},
   "source": [
    "De forma predeterminada, las bibliotecas utilizadas para realizar peticiones HTTP de forma automática establecen su propio *user agent* basándose en el nombre de la librería, facilitándole la tarea a Crunchbase que, por defecto, bloquea el acceso de todos los robots a su página web."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2733cfc",
   "metadata": {},
   "source": [
    "Modificamos el *user agent* y otras cabeceras HTTP para ocultar el hecho de que las peticiones realizadas provienen de un script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b4881d",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,\\\n",
    "    */*;q=0.8\",\n",
    "    \"Accept-Encoding\": \"gzip, deflate, sdch, br\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.8\",\n",
    "    \"Cache-Control\": \"no-cache\",\n",
    "    \"dnt\": \"1\",\n",
    "    \"Pragma\": \"no-cache\",\n",
    "    \"Upgrade-Insecure-Requests\": \"1\",\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) AppleWebKit/5\\\n",
    "    37.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1656849",
   "metadata": {},
   "source": [
    "## Espaciado de peticiones HTTP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76ad7ec",
   "metadata": {},
   "source": [
    "Al principio, nos bastó con modificar el *user agent* para esquivar el bloqueo de peticiones. Lanzábamos peticiones ocasionalmente y nos dedicamos a analizar la estructura de la web para recoger los datos. Una vez finalizada la fase de análisis y diseño del proceso de automatización, y desplegamos la solución, Crunchbase nos denegó el acceso. Nos habían pillado.\n",
    "\n",
    "Entonces, procedimos a implementar la segunda medida prevención: **simular el comportamiento humano estableciendo un tiempo de espera** fijo. Y funcionó. Pero, ya habiendo sido bloqueados dos veces, quisimos adelantarnos a los hechos, y establecer un tiempo de espera aleatorio de entre 5 y 20 segundos para evitar que Crunchbase observara un patrón con facilidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f2164f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url, stream=True, headers=headers)\n",
    "root = etree.fromstring(response.content)\n",
    "rows = []\n",
    "\n",
    "for sitemap in root:\n",
    "    children = sitemap.getchildren()\n",
    "    # filtrar tipo de perfil\n",
    "    #organization = re.match(r\".+organizations-[0-9]+\", children[0].text)\n",
    "    organization = re.match(r\".+organizations-9\", children[0].text)\n",
    "    if organization:\n",
    "        print(children[0].text)\n",
    "        r = requests.get(children[0].text,stream=True, headers=headers)\n",
    "        g=gzip.GzipFile(fileobj=BytesIO(r.content))\n",
    "        content=g.read()\n",
    "        rootorg = etree.fromstring(content)\n",
    "        count = 0\n",
    "        for company in rootorg:\n",
    "            if count > 500:\n",
    "                childrenorg = company.getchildren()\n",
    "                time.sleep(random.randint(5,20))\n",
    "                rlink = requests.get(childrenorg[0].text, headers=headers)\n",
    "                soup = BS(rlink.content, 'html.parser')\n",
    "                # indicar por pantalla si el acceso ha sido denegado, y terminar el proceso\n",
    "                if soup.head.title.text == \"Access to this page has been denied.\":\n",
    "                    print(\"Access denied\")\n",
    "                    break\n",
    "                elif len(rows) >= 1000:\n",
    "                    print(\"Reached limit\")\n",
    "                    break\n",
    "                # obtener la información de la empresa y añadirla a lista de registros\n",
    "                row = parse2row(soup, log=True)\n",
    "                rows.append(row)\n",
    "            else:\n",
    "                count = count + 1\n",
    "        else:\n",
    "            continue\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b29860",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b8e3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832f773c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f92bd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5702490f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"cb_organizations-9_top1000.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
